# Snakemake Workflow Example

This project demonstrates a modular, reproducible data analysis workflow using Snakemake. It is designed to run efficiently on both local machines and high-performance computing clusters (e.g., Great Lakes at the University of Michigan) via SLURM job submission.

**Key Features:**

- Single workflow runs on local and cluster systems
- Explicit resource declarations per rule
- Informative progress messages and logging
- Flexible profile system for different execution environments

## Project Structure

```text
.
├── data/                           # Input data directory
│   └── raw_data.csv               # Sample raw data file
├── logs/                           # Log files generated during workflow execution
├── results/                        # Output files generated by the workflow
└── workflow/                       # Main workflow directory
    ├── Snakefile                   # Primary workflow file (imports rules)
    ├── config/
    │   └── config.yaml             # Workflow configuration (paths, parameters, delimiters)
    ├── envs/
    │   └── smk-ex.yaml             # Conda environment specification
    ├── profiles/
    │   └── slurm/
    │       └── config.yaml         # SLURM cluster execution profile
    ├── rules/
    │   ├── all.smk                 # Final output rule
    │   ├── preprocess.smk          # Data preprocessing rules
    │   └── analysis.smk            # Analysis and visualization rules
    └── scripts/
        ├── __init__.py             # Python package initialization
        └── helpers.py              # Data processing helper functions
```

**Key directories:**

- **workflow/config/**: Contains workflow-level configuration consumed by the Snakefile and rules.
  - `config.yaml`: Defines workflow paths (data, results, logs) and workflow parameters such as the preprocessing delimiter and final target.

- **workflow/envs/**: Contains conda environment specifications.
  - `smk-ex.yaml`: Defines the base conda environment for the example.

- **workflow/rules/**: Contains rule files that define the workflow's execution order and dependencies.
  - `all.smk`: Main rule file that specifies the final output of the workflow.
  - `analysis.smk`: Rules specific to the analysis steps of the workflow.
  - `preprocess.smk`: Rules for preprocessing the data before analysis.

- **workflow/Snakefile**: The main file that orchestrates the Snakemake workflow, importing rules from the `rules` directory and defining the overall workflow structure.

- **workflow/scripts/**: Contains Python helper scripts.
  - `helpers.py`: Core data processing functions called by workflow rules.

- **workflow/profiles/slurm/**: SLURM profile that enables cluster job submission.
  - `config.yaml`: Defines the executor (`slurm`), sets default resources (partition, account, runtime, memory, CPUs), and parallel job limits.

## Setup Instructions

You should alread have a "base" snakemake env per the [snakemake instructions](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).

1. **Environment Setup**: Create the conda environments specified in the `envs` directory using the provided YAML files.

    ```bash
    snakemake --sdm conda --conda-create-envs-only
    ```

1. **Configuration**:

   - Workflow paths and rule parameters: edit `workflow/config/config.yaml`.
   - Cluster resources (partition, account, memory, CPUs, runtime): edit `workflow/profiles/slurm/config.yaml` under `default-resources`.

1. **Running the Workflow**:

   **Local execution (recommended for testing):**

   ```bash
   snakemake --cores 4
   ```

   **SLURM cluster execution:**

   ```bash
   snakemake --profile workflow/profiles/slurm \
   --default-resources slurm_account="support" slurm_partition="build" \
   --sdm conda
   ```

   **Dry run (preview without executing):**

   ```bash
   snakemake --cores 4 --dry-run
   ```

## Usage Guidelines

- **Logs**: Monitor progress and errors in the `logs/` directory. Each rule generates its own log file.
- **Modifications**: Edit rules in the `workflow/rules/` directory to customize the analysis pipeline.
- **Parameters**: Update workflow parameters in `workflow/config/config.yaml` (delimiter, paths, etc.).
- **SLURM Users**: Before first cluster execution, configure `workflow/profiles/slurm/config.yaml`:
  - Set `slurm_account` to your SLURM account (check with `sacctmgr list accounts`)
  - Set `slurm_partition` to your default partition (check with `sinfo`)
- **Progress Tracking**: Each rule includes a `message:` directive that prints when executed, helping you track workflow progress.

## Troubleshooting

**Workflow doesn't run:**

- Check that data files exist in `data/`
- Verify Python scripts in `workflow/scripts/` are executable
- Run `snakemake --cores 4 --dry-run` to identify issues

**SLURM job fails:**

- Check job logs in `logs/slurm_logs/` for error details
- Verify account and partition names match your SLURM configuration
- Increase resource limits in `workflow/profiles/slurm/config.yaml` if jobs timeout
