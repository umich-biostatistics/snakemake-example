# Snakemake Workflow Example

This project demonstrates a simple, modular, reproducible data analysis workflow using Snakemake. It is designed to run on both local machines and high-performance computing clusters (e.g., Great Lakes at the University of Michigan) via the SLURM job submission executor plugin.

For more in-depth examples, check out the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/).

**Key Features:**

- Single workflow runs on local and cluster systems
- Explicit resource declarations per rule
- Informative progress messages and logging
- Flexible profile system for different execution environments

## Project Structure

```text
.
├── config/                       # Configuration files
│   └── config.yaml               # Workflow configuration (paths, parameters, delimiters)
├── data/                         # Input data directory
│   └── raw_data.csv              # Sample raw data file
├── envs/                         # Conda environment specifications
│   └── smk-ex.yaml               # Conda environment specification
├── logs/                         # Log files generated during workflow execution
├── profiles/                     # Execution profiles
│   └── slurm/
│       └── config.yaml           # SLURM cluster execution profile
├── results/                      # Output files generated by the workflow
└── workflow/                     # Main workflow directory
    ├── Snakefile                 # Primary workflow file (imports rules)
    ├── rules/
    │   ├── all.smk               # Final output rule
    │   ├── analysis.smk          # Analysis and visualization rules
    │   └── preprocess.smk        # Data preprocessing rules
    └── scripts/
        ├── __init__.py           # Python package initialization
        └── helpers.py            # Data processing helper functions
```

**Key directories:**

- **config/**: Contains workflow-level configuration consumed by the Snakefile and rules.
  - `config.yaml`: Defines workflow paths (data, results, logs) and workflow parameters such as the preprocessing delimiter and final target.

- **workflow/envs/**: Contains conda environment specifications.
  - `smk-ex.yaml`: Defines the base conda environment for the example.

- **workflow/rules/**: Contains rule files that define the workflow's execution order and dependencies.
  - `all.smk`: Main rule file that specifies the final output of the workflow.
  - `analysis.smk`: Rules specific to the analysis steps of the workflow.
  - `preprocess.smk`: Rules for preprocessing the data before analysis.

- **workflow/Snakefile**: The main file that orchestrates the Snakemake workflow, importing rules from the `rules` directory and defining the overall workflow structure.

- **workflow/scripts/**: Contains Python helper scripts.
  - `helpers.py`: Core data processing functions called by workflow rules.

- **workflow/profiles/slurm/**: SLURM profile that enables cluster job submission.
  - `config.yaml`: Defines the executor (`slurm`), sets default resources (partition, account, runtime, memory, CPUs), and parallel job limits.

## Prerequisites

> [!WARNING]
> The newest version of snakemake (9.14) at the time of writing (2025-12-10) does not work with the slurm executor plugin.
> See issue [#3853](https://github.com/snakemake/snakemake/issues/3853). Downgrade to, or install, snakemake=9.13.

- Conda or Mamba installed on your system.
  - **On U-M clusters**, load a conda/mamba module first with `module load mamba` or similar
- Create a "base" snakemake env per the [snakemake instructions](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).
  - Make sure to activate the base before running and snakemake commands with `source activate snakmakeEnvName`.

## Setup Instructions

1. **Environment Setup**:

   - Create the conda environments specified in the `workflow/envs/` directory:

    ```bash
    snakemake --sdm conda --conda-create-envs-only
    ```

   - The workflow will use this environment when executing rules.

2. **Configuration**:

   - Workflow paths and rule parameters: edit `config/config.yaml`.
   - Cluster resources (partition, account, memory, CPUs, runtime): edit `workflow/profiles/slurm/config.yaml` under `default-resources`.

3. **Running the Workflow**:

   **Local execution (recommended for testing):**

   ```bash
   snakemake --cores 4 --sdm conda
   ```

   **SLURM cluster execution:**

   ```bash
   snakemake --workflow-profile workflow/profiles/slurm \
       --default-resources slurm_account="yourAccount0" slurm_partition="standard" \
       --sdm conda
   ```

   **Dry run (preview without executing):**

   ```bash
   snakemake --cores 4 --dry-run --sdm conda
   ```

> [!NOTE]
> You don't need to pass the `--default-resources` flag if you've edited the rules or `workflow/profiles/slurm/config.yaml` to define these.

## Running on clusters

This workflow can be run on HPC clusters in two common ways. Choose the batch-submission approach for long-running or resource-heavy controller processes, or use a persistent session on the login node for short tests when the cluster policy allows it.

### Option 1: SBATCH

**Batch submit with `smk-slurm.sh` (recommended for long runs)**: Schedule the Snakemake controller as a regular SLURM job so it runs on a compute node with allocated resources.

- Edit the SBATCH header in `smk-slurm.sh` to set your `--account`, `--partition`, `--time`, `--cpus-per-task`, and `--mem`.
- Remove `--forceall` from the Snakemake command in the script for normal incremental runs (leave it only for forced full re-runs). Submit with:

```bash
sbatch smk-slurm.sh
```

The controller's stdout/stderr go to the file defined by `#SBATCH --output=` (default: `logs/controller.log`). Use this method when login nodes prohibit long-lived processes or when you want the controller to run on a compute node.

The provided `smk-slurm.sh` runs Snakemake with recovery-oriented defaults to make restarts safer:

- `--rerun-incomplete` — re-run jobs that left incomplete output after a crash.
- `--restart-times 3` — retry failed jobs up to 3 times for transient errors.
- `--latency-wait 60` — wait up to 60 seconds for files to appear on shared filesystems.

Adjust or remove those flags directly in `smk-slurm.sh` if you prefer a different behavior.

### Option 2: Persistent Session ([Multiplexer](https://effective-shell.com/part-6-advanced-techniques/master-the-multiplexer/))

**Run on a login node inside `tmux`/`screen` (short running/tests)**: For short jobs, tests, or interactive debugging, start a persistent session and run Snakemake from the login node. Example:

```bash
tmux new -s snakemake
# inside the session:
module load mamba
 snakemake --workflow-profile workflow/profiles/slurm \
   --default-resources slurm_account="yourAccount0" slurm_partition="standard" \
   --sdm conda
# detach: Ctrl-b d (tmux) or Ctrl-a d (screen)
```

- Detach the session so the controller survives network disconnects and re-attach later with `tmux attach -t snakemake` (or the equivalent `screen` commands).
- Only use this approach when your cluster's policies permit running the controller on the login node — otherwise prefer the batch script above.

## Usage Guidelines

- **Logs**: Monitor progress and errors in the `logs/` directory. Each rule generates its own log file.
- **Modifications**: Edit rules in the `workflow/rules/` directory to customize the analysis pipeline.
- **Parameters**: Update workflow parameters in `config/config.yaml` (delimiter, paths, etc.).
- **SLURM Users**: Before first cluster execution, configure `workflow/profiles/slurm/config.yaml`:
  - Set `slurm_account` to your SLURM account (check with `sacctmgr list accounts`)
  - Set `slurm_partition` to your default partition (check with `sinfo`)
- **Progress Tracking**: Each rule includes a `message:` directive that prints when executed, helping you track workflow progress.

## Troubleshooting

**Workflow doesn't run:**

- Check that data files exist in `data/`
- Verify Python scripts in `workflow/scripts/` are executable
- Run `snakemake --cores 4 --dry-run` to identify issues

**SLURM job fails:**

- Check job logs in `logs/slurm_logs/` for error details
- Verify account and partition names match your SLURM configuration
- Increase resource limits in `workflow/profiles/slurm/config.yaml` if jobs timeout
